{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yrslyerL-Bh",
        "outputId": "82b34904-77d2-469f-8421-45156670c8bc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# copy dataset from google drive to local storage\n",
        "drive_data_path = '/content/drive/MyDrive/dynamic_res/imagenette2'\n",
        "local_data_path = '/content/imagenette2'\n",
        "\n",
        "if not os.path.exists(local_data_path):\n",
        "    shutil.copytree(drive_data_path, local_data_path)\n",
        "else:\n",
        "    print(f'dataset already exists at {local_data_path}')\n",
        "\n",
        "data_path = local_data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XpDFfHCeMHKN"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import time\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import sys\n",
        "\n",
        "def parse_args():\n",
        "    argv = [arg for arg in sys.argv if not arg.startswith('-f')]\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('data')\n",
        "    parser.add_argument('--arch')\n",
        "    parser.add_argument('--epochs', type=int)\n",
        "    parser.add_argument('--batch-size', type=int)\n",
        "    parser.add_argument('--workers', type=int)\n",
        "    parser.add_argument('--r-min', type=int)\n",
        "    parser.add_argument('--r-max', type=int)\n",
        "    parser.add_argument('--gamma', type=float)\n",
        "    parser.add_argument('--reassign-epoch', type=int)\n",
        "    parser.add_argument('--ratio-schedule')\n",
        "\n",
        "    parser.add_argument('--lr', type=float, default=0.1)\n",
        "    parser.add_argument('--momentum', type=float, default=0.9)\n",
        "    parser.add_argument('--weight-decay', type=float, default=1e-4)\n",
        "    parser.add_argument('--print-freq', type=int, default=10)\n",
        "\n",
        "    return parser.parse_args(argv[1:])\n",
        "\n",
        "class AverageMeter:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "\n",
        "        return res\n",
        "\n",
        "# geometric resolution schedule\n",
        "def compute_resolution_schedule(r_max, r_min, gamma):\n",
        "    schedule = []\n",
        "    r = r_max\n",
        "\n",
        "    while True:\n",
        "        schedule.append(r)\n",
        "        r_next = int(r * gamma)\n",
        "        r_next = max(r_next, r_min)\n",
        "\n",
        "        if r_next == r:\n",
        "            break\n",
        "\n",
        "        r = r_next\n",
        "\n",
        "    return schedule\n",
        "\n",
        "# applies different resolutions to different samples\n",
        "class DynamicResolutionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_dataset, sample_resolutions, normalize):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.sample_resolutions = sample_resolutions\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            img, target = self.base_dataset.samples[idx]\n",
        "            img = self.base_dataset.loader(img)\n",
        "            resolution = self.sample_resolutions.get(idx, 224)\n",
        "\n",
        "            transform = transforms.Compose([transforms.RandomResizedCrop(resolution), transforms.RandomHorizontalFlip(), transforms.ToTensor(), self.normalize])\n",
        "\n",
        "            img = transform(img)\n",
        "\n",
        "            return img, target, idx\n",
        "        except Exception as e:\n",
        "            # if image is corrupted, use next sample\n",
        "            print(f'skipping corrupted image at index {idx}: {e}')\n",
        "\n",
        "            return self.__getitem__((idx + 1) % len(self.base_dataset))\n",
        "\n",
        "class SubsetDynamicResolutionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dynamic_dataset, indices):\n",
        "        self.dynamic_dataset = dynamic_dataset\n",
        "        self.indices = list(indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, subset_idx):\n",
        "        original_idx = self.indices[subset_idx]\n",
        "        return self.dynamic_dataset[original_idx]\n",
        "\n",
        "# group by resolution\n",
        "def collate_by_resolution(batch):\n",
        "    resolution_groups = {}\n",
        "    for img, target, idx in batch:\n",
        "        res = img.shape[-1]\n",
        "\n",
        "        if res not in resolution_groups:\n",
        "            resolution_groups[res] = {'images': [], 'targets': [], 'indices': []}\n",
        "\n",
        "        resolution_groups[res]['images'].append(img)\n",
        "        resolution_groups[res]['targets'].append(target)\n",
        "        resolution_groups[res]['indices'].append(idx)\n",
        "\n",
        "    batches = []\n",
        "    for res, group in resolution_groups.items():\n",
        "        images = torch.stack(group['images'])\n",
        "        targets = torch.tensor(group['targets'])\n",
        "        indices = torch.tensor(group['indices'])\n",
        "\n",
        "        batches.append((images, targets, indices))\n",
        "\n",
        "    return batches\n",
        "\n",
        "def train_epoch(train_loader, model, criterion, optimizer, epoch, device, args, track_loss=False):\n",
        "    losses = AverageMeter('Loss')\n",
        "    top1 = AverageMeter('Acc@1')\n",
        "    top5 = AverageMeter('Acc@5')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_start = time.time()\n",
        "    sample_losses = {} if track_loss else None\n",
        "\n",
        "    for i, batches in enumerate(train_loader):\n",
        "        # handle multiple resolution batches\n",
        "        for images, target, indices in batches:\n",
        "            if track_loss:\n",
        "                indices = indices.cpu().numpy()\n",
        "\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            target = target.to(device, non_blocking=True)\n",
        "\n",
        "            output = model(images)\n",
        "            loss_per_sample = nn.functional.cross_entropy(output, target, reduction='none')\n",
        "            loss = loss_per_sample.mean()\n",
        "\n",
        "            if track_loss:\n",
        "                losses_cpu = loss_per_sample.detach().cpu().numpy()\n",
        "                for idx, sample_loss in zip(indices, losses_cpu):\n",
        "                    if idx not in sample_losses:\n",
        "                        sample_losses[idx] = []\n",
        "\n",
        "                    sample_losses[idx].append(sample_loss)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), images.size(0))\n",
        "            top1.update(acc1[0], images.size(0))\n",
        "            top5.update(acc5[0], images.size(0))\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            print(f'Epoch [{epoch}][{i}/{len(train_loader)}] Loss {losses.avg} Acc@1 {top1.avg} Acc@5 {top5.avg}')\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    if track_loss:\n",
        "        aggregated_losses = {idx: np.mean(losses) for idx, losses in sample_losses.items()}\n",
        "\n",
        "        return losses.avg, top1.avg, top5.avg, epoch_time, aggregated_losses\n",
        "\n",
        "    return losses.avg, top1.avg, top5.avg, epoch_time, None\n",
        "\n",
        "def validate(val_loader, model, criterion, device, args):\n",
        "    losses = AverageMeter('Loss')\n",
        "    top1 = AverageMeter('Acc@1')\n",
        "    top5 = AverageMeter('Acc@5')\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            target = target.to(device, non_blocking=True)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), images.size(0))\n",
        "            top1.update(acc1[0], images.size(0))\n",
        "            top5.update(acc5[0], images.size(0))\n",
        "\n",
        "            if i % args.print_freq == 0:\n",
        "                print(f'Test [{i}/{len(val_loader)}] Loss {losses.avg} Acc@1 {top1.avg} Acc@5 {top5.avg}')\n",
        "\n",
        "    print(f'Val: Acc@1 {top1.avg} Acc@5 {top5.avg}')\n",
        "\n",
        "    return losses.avg, top1.avg, top5.avg\n",
        "\n",
        "def validate_dynamic(val_loader, model, criterion, device, args):\n",
        "    losses = AverageMeter('Loss')\n",
        "    top1 = AverageMeter('Acc@1')\n",
        "    top5 = AverageMeter('Acc@5')\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batches in enumerate(val_loader):\n",
        "            # handle multiple resolution batches\n",
        "            for images, target, indices in batches:\n",
        "                images = images.to(device, non_blocking=True)\n",
        "                target = target.to(device, non_blocking=True)\n",
        "\n",
        "                output = model(images)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "                losses.update(loss.item(), images.size(0))\n",
        "                top1.update(acc1[0], images.size(0))\n",
        "                top5.update(acc5[0], images.size(0))\n",
        "\n",
        "            if i % args.print_freq == 0:\n",
        "                print(f'Test [{i}/{len(val_loader)}] Loss {losses.avg} Acc@1 {top1.avg} Acc@5 {top5.avg}')\n",
        "\n",
        "    print(f'Val: Acc@1 {top1.avg} Acc@5 {top5.avg}')\n",
        "\n",
        "    return losses.avg, top1.avg, top5.avg\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f'resolution range: {args.r_min} - {args.r_max}')\n",
        "    print(f'gamma: {args.gamma}')\n",
        "\n",
        "    resolution_schedule = compute_resolution_schedule(args.r_max, args.r_min, args.gamma)\n",
        "    ratio_schedule = [float(x) for x in args.ratio_schedule.split(',')]\n",
        "\n",
        "    model = models.__dict__[args.arch](num_classes=10)\n",
        "    model = model.to(device)\n",
        "\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "    traindir = os.path.join(args.data, 'train')\n",
        "    valdir = os.path.join(args.data, 'val')\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    base_train_dataset = datasets.ImageFolder(traindir)\n",
        "    initial_num_samples = len(base_train_dataset)\n",
        "    sample_resolutions = {i: args.r_max for i in range(initial_num_samples)}\n",
        "\n",
        "    active_indices = list(range(initial_num_samples))\n",
        "\n",
        "    val_dataset = datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize]))\n",
        "\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "    log_file = f'training_log_{args.arch}_drop.csv'\n",
        "    with open(log_file, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['epoch', 'train_loss', 'train_top1', 'epoch_time', 'num_active_samples'])\n",
        "\n",
        "    print(f'logging to: {log_file}')\n",
        "\n",
        "    total_start = time.time()\n",
        "    cumulative_losses = {}\n",
        "    reassignment_count = 0\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        train_dataset_full = DynamicResolutionDataset(base_train_dataset, sample_resolutions, normalize)\n",
        "        train_dataset = SubsetDynamicResolutionDataset(train_dataset_full, active_indices)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True, collate_fn=collate_by_resolution)\n",
        "\n",
        "        # track loss for next drop\n",
        "        will_reassign = (epoch + 1) % 5 == 0 and epoch >= 4\n",
        "        track_loss = will_reassign\n",
        "\n",
        "        result = train_epoch(train_loader, model, criterion, optimizer, epoch, device, args, track_loss)\n",
        "        train_loss, train_top1, train_top5, epoch_time, sample_losses = result\n",
        "\n",
        "        if sample_losses is not None:\n",
        "            for idx, loss_val in sample_losses.items():\n",
        "                if idx not in cumulative_losses:\n",
        "                    cumulative_losses[idx] = []\n",
        "\n",
        "                cumulative_losses[idx].append(loss_val)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        with open(log_file, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([epoch, train_loss, train_top1.item(), epoch_time, len(active_indices)])\n",
        "\n",
        "        print(f'Epoch {epoch} completed in {epoch_time}s')\n",
        "\n",
        "        # drop samples every reassign_epoch starting from reassign_epoch-1\n",
        "        if (epoch + 1) % args.reassign_epoch == 0 and epoch >= args.reassign_epoch - 1:\n",
        "            print(f'Sample dropping after epoch {epoch}')\n",
        "            avg_losses = {idx: np.mean(losses) for idx, losses in cumulative_losses.items()}\n",
        "\n",
        "            # sort by loss (ascending) and drop easiest samples\n",
        "            sorted_samples = sorted(avg_losses.items(), key=lambda x: x[1])\n",
        "\n",
        "            current_ratio = ratio_schedule[min(reassignment_count, len(ratio_schedule) - 1)]\n",
        "            num_active = len(active_indices)\n",
        "            num_to_drop = int(num_active * current_ratio)\n",
        "\n",
        "            candidates_to_drop = [idx for idx, _ in sorted_samples]\n",
        "            samples_to_drop = candidates_to_drop[:num_to_drop]\n",
        "\n",
        "            if num_to_drop > 0 and len(samples_to_drop) > 0:\n",
        "                drop_set = set(samples_to_drop)\n",
        "                active_indices = [idx for idx in active_indices if idx not in drop_set]\n",
        "\n",
        "            reassignment_count += 1\n",
        "\n",
        "            print(f'current drop ratio: {current_ratio*100}%')\n",
        "            print(f'easiest samples dropped: {len(samples_to_drop)} / {num_active} ({current_ratio*100}%)')\n",
        "            print(f'active samples remaining: {len(active_indices)} / {initial_num_samples}')\n",
        "\n",
        "            cumulative_losses = {}\n",
        "\n",
        "    total_time = time.time() - total_start\n",
        "    print(f'total training time: {total_time}s')\n",
        "\n",
        "    test_loss, test_top1, test_top5 = validate(val_loader, model, criterion, device, args)\n",
        "    print(f'final test results:')\n",
        "    print(f'Test Loss: {test_loss}')\n",
        "    print(f'Test Acc@1: {test_top1}')\n",
        "    print(f'Test Acc@5: {test_top5}')\n",
        "\n",
        "    val_base_dataset = datasets.ImageFolder(valdir)\n",
        "    num_val_samples = len(val_base_dataset)\n",
        "\n",
        "    # assign same resolution distribution as training\n",
        "    res_counts = {}\n",
        "    for res in sample_resolutions.values():\n",
        "        res_counts[res] = res_counts.get(res, 0) + 1\n",
        "\n",
        "    # randomly assign test samples to match training distribution\n",
        "    val_sample_resolutions = {}\n",
        "    train_resolutions = list(sample_resolutions.values())\n",
        "    import random\n",
        "    random.seed(42)\n",
        "    for i in range(num_val_samples):\n",
        "        val_sample_resolutions[i] = random.choice(train_resolutions)\n",
        "\n",
        "    val_dynamic_dataset = DynamicResolutionDataset(val_base_dataset, val_sample_resolutions, normalize)\n",
        "    val_dynamic_loader = DataLoader(val_dynamic_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True, collate_fn=collate_by_resolution)\n",
        "\n",
        "    test_loss_dynamic, test_top1_dynamic, test_top5_dynamic = validate_dynamic(val_dynamic_loader, model, criterion, device, args)\n",
        "    print(f'final test results (dynamic res):')\n",
        "    print(f'Test Loss: {test_loss_dynamic}')\n",
        "    print(f'Test Acc@1: {test_top1_dynamic}')\n",
        "    print(f'Test Acc@5: {test_top5_dynamic}')\n",
        "\n",
        "    with open(log_file, 'a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['FINAL_TEST_224', test_loss, test_top1.item(), total_time, len(active_indices)])\n",
        "        writer.writerow(['FINAL_TEST_DYNAMIC', test_loss_dynamic, test_top1_dynamic.item(), total_time, len(active_indices)])\n",
        "\n",
        "    print(f'results saved to: {log_file}')\n",
        "\n",
        "    # save model to google drive\n",
        "    save_path = '/content/drive/MyDrive/dynamic_res/model_drop.pth'\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f'\\nmodel saved to: {save_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "swBU4XpXMHpS",
        "outputId": "a32ed966-6313-4e66-f258-1e38f0e03073"
      },
      "outputs": [],
      "source": [
        "sys.argv = ['train.py', data_path, '--arch', 'resnet18', '--epochs', '30', '--batch-size', '128', '--workers', '2', '--r-min', '112', '--r-max', '224',\n",
        "            '--gamma', '0.5', '--reassign-epoch', '10', '--ratio-schedule', '0.1,0.2']\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7ACibTOEMJTs",
        "outputId": "2a893a93-98b6-411b-9de3-29f217668790"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_051bbf22-d5f0-4679-869c-e953830e0c5d\", \"training_log_resnet18_drop.csv\", 2122)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('training_log_resnet18_drop.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgMzYZMdvPru",
        "outputId": "c5068c39-1150-44b6-e10c-9a11b862133e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# copy dataset from google drive to local storage\n",
        "drive_data_path = '/content/drive/MyDrive/dynamic_res/imagenette2'\n",
        "local_data_path = '/content/imagenette2'\n",
        "\n",
        "if not os.path.exists(local_data_path):\n",
        "    shutil.copytree(drive_data_path, local_data_path)\n",
        "else:\n",
        "    print(f'dataset already exists at {local_data_path}')\n",
        "\n",
        "data_path = local_data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l08uz-AgZvS"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import time\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import sys\n",
        "\n",
        "def parse_args():\n",
        "    argv = [arg for arg in sys.argv if not arg.startswith('-f')]\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('data')\n",
        "    parser.add_argument('--arch')\n",
        "    parser.add_argument('--epochs', type=int)\n",
        "    parser.add_argument('--batch-size', type=int)\n",
        "    parser.add_argument('--workers', type=int)\n",
        "\n",
        "    parser.add_argument('--lr', type=float, default=0.1)\n",
        "    parser.add_argument('--momentum', type=float, default=0.9)\n",
        "    parser.add_argument('--weight-decay', type=float, default=1e-4)\n",
        "    parser.add_argument('--print-freq', type=int, default=10)\n",
        "\n",
        "    return parser.parse_args(argv[1:])\n",
        "\n",
        "class AverageMeter:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "\n",
        "        return res\n",
        "\n",
        "class RobustImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_dataset):\n",
        "        self.base_dataset = base_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            return self.base_dataset[idx]\n",
        "        except Exception as e:\n",
        "            print(f'skipping corrupted image at index {idx}: {e}')\n",
        "            return self.__getitem__((idx + 1) % len(self.base_dataset))\n",
        "\n",
        "def train_epoch(train_loader, model, criterion, optimizer, epoch, device, args):\n",
        "    losses = AverageMeter('Loss')\n",
        "    top1 = AverageMeter('Acc@1')\n",
        "    top5 = AverageMeter('Acc@5')\n",
        "\n",
        "    model.train()\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        target = target.to(device, non_blocking=True)\n",
        "\n",
        "        output = model(images)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        top1.update(acc1[0], images.size(0))\n",
        "        top5.update(acc5[0], images.size(0))\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            print(f'Epoch [{epoch}][{i}/{len(train_loader)}] Loss {losses.avg} Acc@1 {top1.avg} Acc@5 {top5.avg}')\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    return losses.avg, top1.avg, top5.avg, epoch_time\n",
        "\n",
        "def validate(val_loader, model, criterion, device, args):\n",
        "    losses = AverageMeter('Loss')\n",
        "    top1 = AverageMeter('Acc@1')\n",
        "    top5 = AverageMeter('Acc@5')\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            target = target.to(device, non_blocking=True)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), images.size(0))\n",
        "            top1.update(acc1[0], images.size(0))\n",
        "            top5.update(acc5[0], images.size(0))\n",
        "\n",
        "            if i % args.print_freq == 0:\n",
        "                print(f'Test [{i}/{len(val_loader)}] Loss {losses.avg} Acc@1 {top1.avg} Acc@5 {top5.avg}')\n",
        "\n",
        "    print(f'Val: Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}')\n",
        "    return losses.avg, top1.avg, top5.avg\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'using device: {device}')\n",
        "\n",
        "    model = models.__dict__[args.arch](num_classes=10)\n",
        "    model = model.to(device)\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "    traindir = os.path.join(args.data, 'train')\n",
        "    valdir = os.path.join(args.data, 'val')\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    base_train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize]))\n",
        "    train_dataset = RobustImageFolder(base_train_dataset)\n",
        "\n",
        "    base_val_dataset = datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize]))\n",
        "    val_dataset = RobustImageFolder(base_val_dataset)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "    log_file = f'training_log_{args.arch}_baseline.csv'\n",
        "    with open(log_file, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['epoch', 'train_loss', 'train_top1', 'epoch_time'])\n",
        "\n",
        "    print(f'logging to: {log_file}')\n",
        "    total_start = time.time()\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        train_loss, train_top1, train_top5, epoch_time = train_epoch(train_loader, model, criterion, optimizer, epoch, device, args)\n",
        "        scheduler.step()\n",
        "\n",
        "        with open(log_file, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([epoch, train_loss, train_top1.item(), epoch_time])\n",
        "\n",
        "        print(f'epoch {epoch} completed in {epoch_time}s')\n",
        "\n",
        "    total_time = time.time() - total_start\n",
        "    print(f'total training time: {total_time}s')\n",
        "\n",
        "    # final test set evaluation\n",
        "    test_loss, test_top1, test_top5 = validate(val_loader, model, criterion, device, args)\n",
        "    print(f'final test results:')\n",
        "    print(f'Test Loss: {test_loss}')\n",
        "    print(f'Test Acc@1: {test_top1}')\n",
        "    print(f'Test Acc@5: {test_top5}')\n",
        "\n",
        "    # append final test results to CSV\n",
        "    with open(log_file, 'a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['FINAL_TEST', test_loss, test_top1.item(), total_time])\n",
        "\n",
        "    print(f'results saved to: {log_file}')\n",
        "\n",
        "    # save model to google drive\n",
        "    save_path = '/content/drive/MyDrive/dynamic_res/model_baseline.pth'\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f'\\nmodel saved to: {save_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wObbzLuMiNIQ",
        "outputId": "043f88db-f111-47c2-b7ca-154d0dc706ef"
      },
      "outputs": [],
      "source": [
        "sys.argv = ['train.py', data_path, '--arch', 'resnet18', '--epochs', '30', '--batch-size', '128', '--workers', '2']\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DkfaOgw5vEYw",
        "outputId": "ee6b9bd7-3e44-48d7-8d45-68394c090623"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_f483266b-8994-4fb7-ba36-89ae1cbcc388\", \"training_log_resnet18_baseline.csv\", 1866)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('training_log_resnet18_baseline.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

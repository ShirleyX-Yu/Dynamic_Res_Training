{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdDxa7YLEOza"
      },
      "source": [
        "# Progressive Resizing Baseline\n",
        "**Paper:** https://www.fast.ai/posts/2018-04-30-dawnbench-fastai.html  \n",
        "Appears to be a commonly accepted mechanism for training on smaller images at the start of training, and gradually increasing image size as you train further. Effectively the opposite of dynamic resolution, except all images in dataset are the same size at one time.\n",
        "\n",
        "\n",
        "**Extra Resources:**\n",
        "*   https://miguel-data-sc.github.io/2017-11-23-second/\n",
        "*   https://docs.mosaicml.com/projects/composer/en/stable/method_cards/progressive_resizing.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf0NW00dHpvN",
        "outputId": "0f764be9-a816-4d8c-cab9-ea951d7d4fd6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# copy dataset from google drive to local storage\n",
        "drive_data_path = '/content/drive/MyDrive/dynamic_res/imagenette2'\n",
        "local_data_path = '/content/imagenette2'\n",
        "\n",
        "if not os.path.exists(local_data_path):\n",
        "    shutil.copytree(drive_data_path, local_data_path)\n",
        "else:\n",
        "    print(f'dataset already exists at {local_data_path}')\n",
        "\n",
        "data_path = local_data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srXJCjF2Jbgq"
      },
      "outputs": [],
      "source": [
        "\n",
        "import argparse\n",
        "import time\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import sys\n",
        "\n",
        "def parse_args():\n",
        "    argv = [arg for arg in sys.argv if not arg.startswith('-f')]\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='PyTorch Imagenette Progressive Resizing Training')\n",
        "\n",
        "    parser.add_argument('data', metavar='DIR', help='path to dataset')\n",
        "    parser.add_argument('--arch')\n",
        "    parser.add_argument('--epochs', type=int)\n",
        "    parser.add_argument('--batch-size', type=int)\n",
        "    parser.add_argument('--workers', type=int)\n",
        "    parser.add_argument('--r-min', type=int)\n",
        "    parser.add_argument('--r-start', type=int)\n",
        "    parser.add_argument('--r-max', type=int)\n",
        "    parser.add_argument('--gamma', type=float)\n",
        "    parser.add_argument('--reassign-epoch', type=int)\n",
        "\n",
        "    parser.add_argument('--lr', default=0.1, type=float)\n",
        "    parser.add_argument('--momentum', default=0.9, type=float)\n",
        "    parser.add_argument('--weight-decay', default=1e-4, type=float)\n",
        "    parser.add_argument('--print-freq', default=10, type=int)\n",
        "\n",
        "    return parser.parse_args(argv[1:])\n",
        "\n",
        "class AverageMeter:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "\n",
        "        return res\n",
        "\n",
        "# inverted geometric resolution schedule (starts low, goes high)\n",
        "def compute_resolution_schedule(r_start, r_max, gamma):\n",
        "    schedule = []\n",
        "    r = r_start\n",
        "\n",
        "    while True:\n",
        "        schedule.append(r)\n",
        "        r_next = int(r * gamma)\n",
        "        r_next = min(r_next, r_max)\n",
        "\n",
        "        if r_next == r:\n",
        "            break\n",
        "\n",
        "        r = r_next\n",
        "\n",
        "    return schedule\n",
        "\n",
        "class RobustImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_dataset):\n",
        "        self.base_dataset = base_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            return self.base_dataset[idx]\n",
        "        except Exception as e:\n",
        "            print(f'skipping corrupted image at index {idx}: {e}')\n",
        "            return self.__getitem__((idx + 1) % len(self.base_dataset))\n",
        "\n",
        "def train_epoch(train_loader, model, criterion, optimizer, epoch, device, args):\n",
        "    losses = AverageMeter('Loss')\n",
        "    top1 = AverageMeter('Acc@1')\n",
        "    top5 = AverageMeter('Acc@5')\n",
        "\n",
        "    model.train()\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        target = target.to(device, non_blocking=True)\n",
        "\n",
        "        output = model(images)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        top1.update(acc1[0], images.size(0))\n",
        "        top5.update(acc5[0], images.size(0))\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            print(f'Epoch [{epoch}][{i}/{len(train_loader)}] Loss {losses.avg} Acc@1 {top1.avg} Acc@5 {top5.avg}')\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    return losses.avg, top1.avg, top5.avg, epoch_time\n",
        "\n",
        "def validate(val_loader, model, criterion, device, args):\n",
        "    losses = AverageMeter('Loss')\n",
        "    top1 = AverageMeter('Acc@1')\n",
        "    top5 = AverageMeter('Acc@5')\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            target = target.to(device, non_blocking=True)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), images.size(0))\n",
        "            top1.update(acc1[0], images.size(0))\n",
        "            top5.update(acc5[0], images.size(0))\n",
        "\n",
        "            if i % args.print_freq == 0:\n",
        "                print(f'Test [{i}/{len(val_loader)}] Loss {losses.avg} Acc@1 {top1.avg} Acc@5 {top5.avg}')\n",
        "\n",
        "    print(f'Val: Acc@1 {top1.avg} Acc@5 {top5.avg}')\n",
        "    return losses.avg, top1.avg, top5.avg\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f'resolution range: {args.r_start} - {args.r_max}')\n",
        "    print(f'gamma: {args.gamma}')\n",
        "\n",
        "    resolution_schedule = compute_resolution_schedule(args.r_start, args.r_max, args.gamma)\n",
        "    current_res_idx = 0\n",
        "\n",
        "    model = models.__dict__[args.arch](num_classes=10)\n",
        "    model = model.to(device)\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "    traindir = os.path.join(args.data, 'train')\n",
        "    valdir = os.path.join(args.data, 'val')\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    base_train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(resolution_schedule[current_res_idx]), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize]))\n",
        "    train_dataset = RobustImageFolder(base_train_dataset)\n",
        "\n",
        "    base_val_dataset = datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize]))\n",
        "    val_dataset = RobustImageFolder(base_val_dataset)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "    log_file = f'training_log_{args.arch}_progressive.csv'\n",
        "    with open(log_file, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['epoch', 'train_loss', 'train_top1', 'epoch_time'])\n",
        "\n",
        "    print(f'logging to: {log_file}')\n",
        "    total_start = time.time()\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        train_loss, train_top1, train_top5, epoch_time = train_epoch(train_loader, model, criterion, optimizer, epoch, device, args)\n",
        "        scheduler.step()\n",
        "\n",
        "        with open(log_file, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([epoch, train_loss, train_top1.item(), epoch_time])\n",
        "\n",
        "        print(f'epoch {epoch} completed in {epoch_time}s')\n",
        "\n",
        "        # progressive resizing -------------------------------------------------\n",
        "        # reassign resolutions every reassign_epoch starting from reassign_epoch-1\n",
        "        if (epoch + 1) % args.reassign_epoch == 0 and epoch >= args.reassign_epoch - 1:\n",
        "\n",
        "            # get current resolution level and move to next higher resolution\n",
        "            current_res_idx = min(current_res_idx + 1, len(resolution_schedule) - 1)\n",
        "            base_train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(resolution_schedule[current_res_idx]), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize]))\n",
        "            train_dataset = RobustImageFolder(base_train_dataset)\n",
        "            train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
        "        # progressive resizing -------------------------------------------------\n",
        "\n",
        "    total_time = time.time() - total_start\n",
        "    print(f'\\ntotal training time: {total_time}s')\n",
        "\n",
        "    # final test set evaluation\n",
        "    test_loss, test_top1, test_top5 = validate(val_loader, model, criterion, device, args)\n",
        "    print(f'\\nfinal test results:')\n",
        "    print(f'Test Loss: {test_loss}')\n",
        "    print(f'Test Acc@1: {test_top1}')\n",
        "    print(f'Test Acc@5: {test_top5}')\n",
        "\n",
        "    # append final test results to CSV\n",
        "    with open(log_file, 'a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['FINAL_TEST', test_loss, test_top1.item(), total_time])\n",
        "\n",
        "    print(f'results saved to: {log_file}')\n",
        "\n",
        "    # save model to google drive\n",
        "    save_path = '/content/drive/MyDrive/dynamic_res/model_progressive.pth'\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f'\\nmodel saved to: {save_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVGr6gbiJrS3",
        "outputId": "472a9ea5-ab0a-4533-bf6e-c44bcefa9ed5"
      },
      "outputs": [],
      "source": [
        "sys.argv = ['train.py', data_path, '--arch', 'resnet18', '--epochs', '30', '--batch-size', '128', '--workers', '2', '--r-min', '112', '--r-start', '112', '--r-max', '224',\n",
        "            '--gamma', '2.0', '--reassign-epoch', '10']\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "bhl7LNAJJs1z",
        "outputId": "1292656b-a2c4-41f9-c082-15536b20ce4e"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_6b43503f-0d44-47c5-b586-15c736946d13\", \"training_log_resnet18_progressive.csv\", 1874)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('training_log_resnet18_progressive.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

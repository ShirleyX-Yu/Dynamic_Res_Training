{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6V4GXWi9L4i"
   },
   "source": [
    "# Mixed-Precision Training Baseline\n",
    "\n",
    "**Paper:** https://arxiv.org/pdf/1710.03740  \n",
    "\"Reduced precision addresses two of these limiters. Memory bandwidth pressure is lowered by using fewer bits to to store the same number of values. Arithmetic time can also be lowered on processors that offer higher throughput for reduced precision math. For example, half-precision math throughput in recent GPUs is 2% to 8% higher than for single-precision. In addition to speed improvements, reduced precision formats also reduce the amount of memory required for training.\"\n",
    "\n",
    "A proclaimed \"universal\" baseline.\n",
    "\n",
    "**Extra Resources:**\n",
    "*   https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/\n",
    "*   https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/\n",
    "*   https://github.com/NVIDIA/DeepLearningExamples\n",
    "*   https://github.com/Project-MONAI/tutorials/blob/main/acceleration/automatic_mixed_precision.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 428006,
     "status": "ok",
     "timestamp": 1765302221808,
     "user": {
      "displayName": "Cheryl Li",
      "userId": "14243701283001645169"
     },
     "user_tz": 300
    },
    "id": "LgMzYZMdvPru",
    "outputId": "5742d27d-2279-42e0-c970-de5a8b126979"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# copy dataset from google drive to local storage\n",
    "drive_data_path = '/content/drive/MyDrive/dynamic_res/imagenette2'\n",
    "local_data_path = '/content/imagenette2'\n",
    "\n",
    "if not os.path.exists(local_data_path):\n",
    "    shutil.copytree(drive_data_path, local_data_path)\n",
    "else:\n",
    "    print(f'dataset already exists at {local_data_path}')\n",
    "\n",
    "data_path = local_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9793,
     "status": "ok",
     "timestamp": 1765302231607,
     "user": {
      "displayName": "Cheryl Li",
      "userId": "14243701283001645169"
     },
     "user_tz": 300
    },
    "id": "2l08uz-AgZvS"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "\n",
    "def parse_args():\n",
    "    argv = [arg for arg in sys.argv if not arg.startswith('-f')]\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('data')\n",
    "    parser.add_argument('--arch')\n",
    "    parser.add_argument('--epochs', type=int)\n",
    "    parser.add_argument('--batch-size', type=int)\n",
    "    parser.add_argument('--workers', type=int)\n",
    "\n",
    "    parser.add_argument('--lr', default=0.1, type=float)\n",
    "    parser.add_argument('--momentum', default=0.9, type=float)\n",
    "    parser.add_argument('--weight-decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--print-freq', default=10, type=int)\n",
    "\n",
    "    return parser.parse_args(argv[1:])\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "\n",
    "        return res\n",
    "\n",
    "class RobustImageFolder(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            return self.base_dataset[idx]\n",
    "        except Exception as e:\n",
    "            print(f'skipping corrupted image at index {idx}: {e}')\n",
    "            return self.__getitem__((idx + 1) % len(self.base_dataset))\n",
    "\n",
    "def train_epoch(train_loader, model, criterion, optimizer, scaler, epoch, device, args):\n",
    "    losses = AverageMeter('Loss')\n",
    "    top1 = AverageMeter('Acc@1')\n",
    "    top5 = AverageMeter('Acc@5')\n",
    "\n",
    "    model.train()\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        # automatic mixed precision --------------------------------------------\n",
    "        with torch.autocast(\"cuda\", dtype=torch.float16):\n",
    "          output = model(images)\n",
    "          loss = criterion(output, target)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # automatic mixed precision --------------------------------------------\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print(f'Epoch [{epoch}][{i}/{len(train_loader)}] Loss {losses.avg} Acc@1 {top1.avg} Acc@5 {top5.avg}')\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg, epoch_time\n",
    "\n",
    "def validate(val_loader, model, criterion, device, args):\n",
    "    losses = AverageMeter('Loss')\n",
    "    top1 = AverageMeter('Acc@1')\n",
    "    top5 = AverageMeter('Acc@5')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "\n",
    "            # automatic mixed precision ----------------------------------------\n",
    "            with torch.autocast(\"cuda\", dtype=torch.float16):\n",
    "              output = model(images)\n",
    "              loss = criterion(output, target)\n",
    "            # automatic mixed precision ----------------------------------------\n",
    "\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                print(f'Test [{i}/{len(val_loader)}] Loss {losses.avg} Acc@1 {top1.avg} Acc@5 {top5.avg}')\n",
    "\n",
    "    print(f'Val: Acc@1 {top1.avg} Acc@5 {top5.avg}')\n",
    "    return losses.avg, top1.avg, top5.avg\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = models.__dict__[args.arch](num_classes=10)\n",
    "    model = model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    # automatic mixed precision ------------------------------------------------\n",
    "    scaler = torch.amp.GradScaler(\"device\")\n",
    "    # automatic mixed precision ------------------------------------------------\n",
    "\n",
    "    traindir = os.path.join(args.data, 'train')\n",
    "    valdir = os.path.join(args.data, 'val')\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    base_train_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize]))\n",
    "    train_dataset = RobustImageFolder(base_train_dataset)\n",
    "\n",
    "    base_val_dataset = datasets.ImageFolder(valdir, transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize]))\n",
    "    val_dataset = RobustImageFolder(base_val_dataset)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    log_file = f'training_log_{args.arch}_precision.csv'\n",
    "    with open(log_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['epoch', 'train_loss', 'train_top1', 'epoch_time'])\n",
    "\n",
    "    print(f'logging to: {log_file}')\n",
    "    total_start = time.time()\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        # automatic mixed precision --------------------------------------------\n",
    "        train_loss, train_top1, train_top5, epoch_time = train_epoch(train_loader, model, criterion, optimizer, scaler, epoch, device, args)\n",
    "        # automatic mixed precision --------------------------------------------\n",
    "        val_loss, val_top1, val_top5 = validate(val_loader, model, criterion, device, args)\n",
    "        scheduler.step()\n",
    "\n",
    "        with open(log_file, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([epoch, train_loss, train_top1.item(), epoch_time])\n",
    "\n",
    "        print(f'epoch {epoch} completed in {epoch_time}s')\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    print(f'\\ntotal training time: {total_time}s')\n",
    "\n",
    "    # final test set evaluation\n",
    "    test_loss, test_top1, test_top5 = validate(val_loader, model, criterion, device, args)\n",
    "    print(f'\\nfinal test results:')\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    print(f'Test Acc@1: {test_top1}')\n",
    "    print(f'Test Acc@5: {test_top5}')\n",
    "\n",
    "    # append final test results to CSV\n",
    "    with open(log_file, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['FINAL_TEST', test_loss, test_top1.item(), total_time])\n",
    "\n",
    "    print(f'results saved to: {log_file}')\n",
    "\n",
    "    # save model to google drive\n",
    "    save_path = '/content/drive/MyDrive/dynamic_res/model_precision.pth'\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f'\\nmodel saved to: {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "wObbzLuMiNIQ"
   },
   "outputs": [],
   "source": [
    "sys.argv = ['train.py', data_path, '--arch', 'resnet18', '--epochs', '30', '--batch-size', '128', '--workers', '2']\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DkfaOgw5vEYw"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_e83679bf-acca-4fb1-a908-01bac7fc587f\", \"training_log_resnet18_precision.csv\", 1870)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download('training_log_resnet18_precision.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
